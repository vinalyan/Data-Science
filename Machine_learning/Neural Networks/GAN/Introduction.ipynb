{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes my notes extracted from the references in order to understand how Generative Adversarial Networks work, a lot of this production is not mine and I would like reference the credit for the authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "\n",
    "This is a kind of reinforcement learning and unsupervised learning where two models play together. We will have a real dataset R, a model __G__ knows as *generator* which is going to create fake data that looks just like the genuine data, while **D** is the *discriminator* who is going to evaluate and calculate the differente between the real set and the set that was modified, finally, we are going to have the *I* random noise that goes into the generator as source of entropy.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1198/1*-gFsbymY9oJUQJ-A3GTfeg.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R, we are going to start with the simplest possible **R**. This function takes a mean and a standard deviation and returns which provides the right shape of a sample data from a Gaussian with those parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, given a set of data instances X and a set of labels Y, we are going to have }\n",
    "+ _Generative_ models capture the joind probability P(X, Y), or just P(X) if there are no labels\n",
    "+ _Discriminative_ models capture the conditional probability P(Y|X)\n",
    "\n",
    "<img src=\"https://developers.google.com/machine-learning/gan/images/generative_v_discriminative.png\" >\n",
    "\n",
    "This is the general architecture of the system\n",
    "\n",
    "<img src=\"https://developers.google.com/machine-learning/gan/images/gan_diagram.svg\" >\n",
    "\n",
    "Both the generator and the discrimator are neuronal networks, where the discriminator is a classifier which knows the distribution of the data and how to segment it, in the same way, it has a backpropation which uses the loss from the interactions. \n",
    "\n",
    "<img src=\"https://developers.google.com/machine-learning/gan/images/gan_diagram_discriminator.svg\">\n",
    "\n",
    "During the discriminator's training, the discriminator ignores the generator's loss and uses the discriminator's loss, in the same way, during the generator's training the generator uses its loss.\n",
    "\n",
    "The discriminator is going to focus to classify both real data and fake from the generator, moreover, the discriminator's loss is going to penalize the discriminator for misclassifiying a real instance as fake or a fake instance as real, finally, the discriminator updates its weights using backpropagation from the discrimator's loss. \n",
    "\n",
    "The generator is focuses to make fake data using the feedback from the discriminator, in other words, it learns from it in order to improve its output to be the more real. \n",
    "\n",
    "G is integrated by the random input _I_, a neural network, the discriminator's output, generator's loss which penalizes the generator for failing to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *loss functions* use in a GAN are:\n",
    "+ __minmax loss__\n",
    "+ __Wasserstein loss__\n",
    "\n",
    "Due to we have two neural nets we can have two loss functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Variations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ *Progressive GANs*, it's generator's first layers produce very low resolution images, and subsequent layers and details. This a kind of technique faster than the others. \n",
    "\n",
    "+ *Conditional GANs*, in this technique we can specify the label for each generated instance, completely different from *unconditional GANs* where it produces random outputs. \n",
    "\n",
    "+ *Image-to-Image translation* it takes an image as input and map it to a generated output image with different properties. \n",
    "\n",
    "+ *CycleGAN* it lerans to transform images from one set into images that could belong to another set. \n",
    "\n",
    "+ *Text-to-Image Synthesis* takes text as input and produce images that plausible and described by the text. \n",
    "\n",
    "+ *Super-resolution* increases the resolution of images, adding detail where necessary to fill in blurry areas. \n",
    "\n",
    "+ *Face inpainting* the idea of this approach is to fill chunks of an image that are blacked out.\n",
    "\n",
    "+ *Text-to-Speech* some researches explored the application of GANs to produce synthetized speech from text input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-GAN tutorial from Google Developers\n",
    "\n",
    "The idea of this tutorial is to use a TF-GAN in the MNIST datasets to make fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluating generative models, and evaluating GANs**\n",
    "\n",
    "tf_scan provides some standard methods of evaluating generative models, we measure:\n",
    "+ Inception Score: called `mnist_score`\n",
    "+ Frechet Inception Distance\n",
    "\n",
    "The idea is using the pre-trained classifier to both the real data and the generated data calculate the *Inception Score*, where its idea is measure both __quality__ and __diversity__ [4].\n",
    "\n",
    "On the other hand, we can find the *Frechet Inception Distance* [5] which calculates how close the generated image distribution is to the real image distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0207 16:59:30.960190 22596 module_wrapper.py:138] From D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensorflow-gan is a library for training and evaluating GANs\n",
    "import tensorflow as tf\n",
    "import tensorflow_gan as tfgan\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as pd \n",
    "%matplotlib inline\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random inputs for the generator  \n",
    "def input_fn(mode, params):\n",
    "    # assertions are statemets that assert or state a fact confidently, they are simply boolean expressions \n",
    "    assert \"batch_size\" in params\n",
    "    assert \"noise_dims\" in params\n",
    "    bs = params[\"batch_size\"]\n",
    "    nd = params[\"noise_dims\"]\n",
    "    split = \"train\" if mode == tf.estimator.ModeKeys.TRAIN else \"test\"\n",
    "    shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    noise_ds = (tf.data.Dataset.from_tensors(0).repeat()\n",
    "              .map(lambda _: tf.random.normal([bs, nd])))\n",
    "    \n",
    "    if just_noise:\n",
    "        return noise_ds\n",
    "    \n",
    "    def _preprocess(element):\n",
    "        # Map [0, 255] to [-1, 1]\n",
    "        images = (tf.cast(element[\"image\"], tf.float32) - 127.5) / 127.5\n",
    "        return images\n",
    "    \n",
    "    images_ds =  (tfds.load(\"mnist\", split=split).map(_preprocess).cache().repeat())\n",
    "    \n",
    "    if shuffle:\n",
    "        images_ds = images_ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "        images_ds = (images_ds.batch(bs, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "        \n",
    "    return tf.data.Dataset.zip((noise_ds, images_ds))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function input_fn.<locals>.<lambda> at 0x000001C8863998C8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "              .map(lambda _: tf.random.normal([bs, nd])))\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "params = {\"batch_size\": 100, \"noise_dims\":64}\n",
    "with tf.Graph().as_default():\n",
    "    ds = input_fn(tf.estimator.ModeKeys.TRAIN, params)\n",
    "    numpy_imgs = next(tfds.as_numpy(ds))[1]\n",
    "img_grid = tfgan.eval.python_image_grid(numpy_imgs, grid_shape=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.squeeze(img_grid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network architecture \n",
    "\n",
    "# a generator that takes input noise and outputs generated MNIST digits\n",
    "# a discriminator that takes images and outputs a probability of being real or fake\n",
    "\n",
    "def _dense(inputs, units, l2_weight):\n",
    "      return tf.keras.layers.Dense(\n",
    "      inputs, units, None,\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "        \n",
    "def _batch_norm(inputs, is_training):\n",
    "      return tf.keras.layers.batch_normalization(\n",
    "      inputs, momentum=0.999, epsilon=0.001, training=is_training)\n",
    "\n",
    "def _deconv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
    "      return tf.keras.layers.conv2d_transpose(\n",
    "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \n",
    "      activation=tf.nn.relu, padding='same',\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "\n",
    "def _conv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
    "      return tf.keras.layers.conv2d(\n",
    "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \n",
    "      activation=None, padding='same',\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unconditional_generator(noise, mode, weight_decay=2.5e-5):\n",
    "    \"\"\"Generator to produce unconditional MNIST images.\"\"\"\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    net = _dense(noise, 1024, weight_decay)\n",
    "    net = _batch_norm(net, is_training)\n",
    "    net = tf.nn.relu(net)\n",
    "\n",
    "    net = _dense(net, 7 * 7 * 256, weight_decay)\n",
    "    net = _batch_norm(net, is_training)\n",
    "    net = tf.nn.relu(net)\n",
    "\n",
    "    net = tf.reshape(net, [-1, 7, 7, 256])\n",
    "    net = _deconv2d(net, 64, 4, 2, weight_decay)\n",
    "    net = _deconv2d(net, 64, 4, 2, weight_decay)\n",
    "    # Make sure that generator output is in the same range as `inputs`\n",
    "    # ie [-1, 1].\n",
    "    net = _conv2d(net, 1, 4, 1, 0.0)\n",
    "    net = tf.tanh(net)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "\n",
    "def unconditional_discriminator(img, unused_conditioning, mode, weight_decay=2.5e-5):\n",
    "    is_training= (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    net = _conv2d(img, 64, 4, 2, weight_decay)\n",
    "    net = _leaky_relu(net)\n",
    "    \n",
    "    net = _conv2d(net, 128, 4, 2, weight_decay)\n",
    "    net = _leaky_relu(net)\n",
    "    \n",
    "    net = tf.keras.layers.flatten(net)\n",
    "    \n",
    "    net = _dense(net, 1024, weight_decay)\n",
    "    net = _batch_norm(net, is_training)\n",
    "    net = _leaky_relu(net)\n",
    "    \n",
    "    net = _dense(net, 1, weight_decay)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_gan.examples.mnist import util as eval_util\n",
    "import os\n",
    "\n",
    "# evaluation function\n",
    "def get_eval_metric_ops_fn(gan_model):\n",
    "    real_data_logits = tf.reduce_mean(gan_model.discriminator_real_outputs)\n",
    "    gen_data_logits = tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
    "    real_mnist_logits = eval_util.mnist_score(gan_model.real_data)\n",
    "    # inception score\n",
    "    generated_mnist_score = eval_util.mnist_score(gan_model.generated_data)\n",
    "    # frechet inception distance\n",
    "    frechet_distance =  eval_util.mnist_frechet_distance(gan_model.real_data, gan_model.generated_data)\n",
    "    return {\n",
    "        'real_data_logits': tf.metrics.mean(real_data_logits),\n",
    "        'gen_data_logits': tf.metrics.mean(gen_data_logits),\n",
    "        'real_mnist_socre': tf.metrics.mean(real_mnist_score),\n",
    "        'mnist_score': tf.metrics.mean(generated_mnist_score),\n",
    "        'frechet_distance': tf.metrics.mean(frechet_distance)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANEstimator assembles and manages the pieces of the whole GAN model, where it's contructor takes the following components:\n",
    "+ Network builder functions\n",
    "+ Loss functions\n",
    "+ Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size =  32 \n",
    "noise_dimensions = 64\n",
    "generator_lr = 0.001\n",
    "discriminator_lr = 0.0002\n",
    "\n",
    "def gen_opt():\n",
    "    gstep =  tf.train.get_or_create_step()\n",
    "    base_lr = generator_lr\n",
    "    lr = tf.cond(gstep < 1000, lambda: base_lr, lambda: base_lr / 2.0)\n",
    "    return tf.optimizers.Adam(lr, 0.5)\n",
    "\n",
    "gan_estimator = tfgan.estimator.GANEstimator(\n",
    "    generator_fn=unconditional_generator,\n",
    "    discriminator_fn=unconditional_discriminator,\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    params={'batch_size': train_batch_size, 'noise_dims': noise_dimensions},\n",
    "    generator_optimizer=gen_opt,\n",
    "    discriminator_optimizer=tf.optimizers.Adam(discriminator_lr, 0.5),\n",
    "    get_eval_metric_ops_fn=get_eval_metric_ops_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train and eval loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-71facb396098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mgan_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0msteps_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_step\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcur_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1162\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1192\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[1;32m-> 1194\u001b[1;33m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[0;32m   1195\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\gan_estimator.py\u001b[0m in \u001b[0;36m_model_fn\u001b[1;34m(features, labels, mode, params)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[1;31m# Make GANModel, which encapsulates the GAN model architectures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m       gan_model = get_gan_model(mode, generator_fn, discriminator_fn, real_data,\n\u001b[1;32m--> 191\u001b[1;33m                                 generator_inputs, add_summaries)\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m       \u001b[1;31m# Make GANLoss, which encapsulates the losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\gan_estimator.py\u001b[0m in \u001b[0;36mget_gan_model\u001b[1;34m(mode, generator_fn, discriminator_fn, real_data, generator_inputs, add_summaries, generator_scope, discriminator_scope)\u001b[0m\n\u001b[0;32m    249\u001b[0m     gan_model = _make_gan_model(generator_fn, discriminator_fn, real_data,\n\u001b[0;32m    250\u001b[0m                                 \u001b[0mgenerator_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                                 discriminator_scope, add_summaries, mode)\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\gan_estimator.py\u001b[0m in \u001b[0;36m_make_gan_model\u001b[1;34m(generator_fn, discriminator_fn, real_data, generator_inputs, generator_scope, discriminator_scope, add_summaries, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m       \u001b[0mgenerator_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m       \u001b[0mdiscriminator_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscriminator_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m       check_shapes=False)\n\u001b[0m\u001b[0;32m    272\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_summaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_summaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\train.py\u001b[0m in \u001b[0;36mgan_model\u001b[1;34m(generator_fn, discriminator_fn, real_data, generator_inputs, generator_scope, discriminator_scope, check_shapes)\u001b[0m\n\u001b[0;32m    105\u001b[0m       generator_scope, reuse=tf.compat.v1.AUTO_REUSE) as gen_scope:\n\u001b[0;32m    106\u001b[0m     \u001b[0mgenerator_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_tensor_or_l_or_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mgenerated_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m   with tf.compat.v1.variable_scope(\n\u001b[0;32m    109\u001b[0m       discriminator_scope, reuse=tf.compat.v1.AUTO_REUSE) as dis_scope:\n",
      "\u001b[1;32m<ipython-input-7-6492ae64296a>\u001b[0m in \u001b[0;36munconditional_generator\u001b[1;34m(noise, mode, weight_decay)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mis_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f55c39bb3b76>\u001b[0m in \u001b[0;36m_dense\u001b[1;34m(inputs, units, l2_weight)\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglorot_uniform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m       \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml2_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m       bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m         activity_regularizer=regularizers.get(activity_regularizer), **kwargs)\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'"
     ]
    }
   ],
   "source": [
    "tf.autograph.set_verbosity(0, False)\n",
    "\n",
    "import time \n",
    "steps_per_eval = 500\n",
    "max_train_steps =  5000\n",
    "batches_for_eval_metrics = 100\n",
    "\n",
    "# used to track metrics\n",
    "steps = []\n",
    "real_logits, fake_logits = [], []\n",
    "real_mnist_scores, mnist_scores, frechet_distances = [], [], []\n",
    "\n",
    "cur_step =  0\n",
    "start_time =  time.time()\n",
    "while cur_step < max_train_steps:\n",
    "    next_step = min(cur_step + steps_per_eval, max_train_steps)\n",
    "    \n",
    "    start = time.time()\n",
    "    gan_estimator.train(input_fn, max_steps=next_step)\n",
    "    steps_taken = next_step - cur_step\n",
    "    time_taken =  time.time() - start\n",
    "    \n",
    "    print(\"Time since start: %.2f min\" %((time.time() - start_time / 60.0)))\n",
    "    print(\"Trained from step %i to %i in %.2f steps / sec\" %(cur_step, next_step, steps_taken / time_taken))\n",
    "cur_step = next_step\n",
    "\n",
    "# calculate some metrics\n",
    "\n",
    "metrics = gan_estimator.evaluate(input_fn, steps=batches_for_eval_metrics)\n",
    "steps.append(cur_step)\n",
    "real_logits.append(metrics[\"real_data_logits\"])\n",
    "fake_logits.append(metrics[\"gen_data_logits\"])\n",
    "real_mnist_scores.append(metrics[\"real_mnist_score\"])\n",
    "mnist_scores.append(metrics[\"frechet_distance\"])\n",
    "print(\"Average discriminator output on Real: %.2f Fake: %.2f %\" % (real_logits[-1], fake_logits[-1]))\n",
    "print(\"Inception Score: %.2f / %.2f Frechet Distance: %.2f\" %(mnist_scores[-1], real_mnist_scores[-1], frechet_distances[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GAN in depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mean of 4.0 and a standard deviation of 1.25\n",
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "G, the generator is a standard feedforward graph - two hidden layers, three linear maps. The idea is G is going to receive the *uniformly* distributed data samples from *I* and somehow replicate the *normally* distributed samples from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 14s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.836386, acc.: 51.56%] [G loss: 0.870476]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-74dcf2b0c332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-74dcf2b0c332>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;31m# If at save interval => save generated image samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msample_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-74dcf2b0c332>\u001b[0m in \u001b[0;36msample_images\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images/%d.png\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(self, fname, **kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2035\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[0;32m   2261\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2263\u001b[1;33m                 **kwargs)\n\u001b[0m\u001b[0;32m   2264\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[0;32m    528\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[1;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m     \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[1;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m         \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seek'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/0.png'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GAN\n",
    "\n",
    "\n",
    "This approach [6] \n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: No se puede encontrar el mdulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a109adcd3909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mshufflenetv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfaster_rcnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmask_rcnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkeypoint_rcnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmisc\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmisc_nn_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiScaleRoIAlign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\ops\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_iou\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mroi_align\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroi_align\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRoIAlign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mroi_pool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroi_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRoIPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpoolers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiScaleRoIAlign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfeature_pyramid_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeaturePyramidNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\ops\\boxes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_C\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: No se puede encontrar el mdulo especificado."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "opt = parser.parse_args()\n",
    "print(opt)\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ [1] https://skymind.ai/wiki/generative-adversarial-network-gan\n",
    "+ [2] https://forum.faceswap.dev/viewtopic.php?t=146\n",
    "+ [3] https://developers.google.com/machine-learning/gan\n",
    "+ [4] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016). Improved techniques for training gans. In Advances in neural information processing systems (pp. 2234-2242).\n",
    "+ [5] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems (pp. 6626-6637).\n",
    "+ [6] Mirza, M., & Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.\n",
    "+ [7] https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
